{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect Real Market Data\n",
    "import yfinance as yf\n",
    "\n",
    "# Download S&P 500 historical data\n",
    "data = yf.download(\"^GSPC\", start=\"2010-01-01\", end=\"2023-01-01\")\n",
    "data.to_csv(\"sp500.csv\")  # Save for later use\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess the Data\n",
    "# Preprocess the data by normalizing it and converting it into a format suitable for training a GAN.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"sp500.csv\", index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "# Use 'Close' prices for training\n",
    "prices = data[[\"Close\"]].values\n",
    "\n",
    "# Normalize data to range [0, 1]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_prices = scaler.fit_transform(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train a GAN for Synthetic Data Generation\n",
    "# Train a GAN to generate synthetic time series data that mimics the S&P 500 closing prices.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, LeakyReLU, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "# Define the GAN generator\n",
    "def build_generator():\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=100),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(),\n",
    "        Dense(1, activation='tanh')  # Output a single value (scaled price)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the GAN discriminator\n",
    "def build_discriminator():\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=1),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification (real/fake)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Combine the GAN\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = Input(shape=(100,))\n",
    "fake_data = generator(gan_input)\n",
    "gan_output = discriminator(fake_data)\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))\n",
    "\n",
    "# Train the GAN\n",
    "epochs = 10000\n",
    "batch_size = 32\n",
    "for epoch in range(epochs):\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    fake_prices = generator.predict(noise)\n",
    "    real_prices = scaled_prices[np.random.randint(0, len(scaled_prices), batch_size)]\n",
    "    discriminator_loss_real = discriminator.train_on_batch(real_prices, np.ones((batch_size, 1)))\n",
    "    discriminator_loss_fake = discriminator.train_on_batch(fake_prices, np.zeros((batch_size, 1)))\n",
    "    discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
    "    generator_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss}, Generator Loss: {generator_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Function for Saving   \n",
    "def save_models_and_data(generator, discriminator, gan, synthetic_data, scaler):\n",
    "    generator.save(\"generator_model.h5\")\n",
    "    discriminator.save(\"discriminator_model.h5\")\n",
    "    gan.save(\"gan_model.h5\")\n",
    "    np.save(\"synthetic_data.npy\", synthetic_data)\n",
    "    with open(\"scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Function for Loading\n",
    "def load_models_and_data():\n",
    "    generator = load_model(\"generator_model.h5\")\n",
    "    discriminator = load_model(\"discriminator_model.h5\")\n",
    "    gan = load_model(\"gan_model.h5\")\n",
    "    synthetic_data = np.load(\"synthetic_data.npy\")\n",
    "    with open(\"scaler.pkl\", \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "    return generator, discriminator, gan, synthetic_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# Save everything\n",
    "save_models_and_data(generator, discriminator, gan, synthetic_data, scaler)\n",
    "\n",
    "# Load everything\n",
    "generator, discriminator, gan, synthetic_data, scaler = load_models_and_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Simulate Extreme Scenarios\n",
    "# Modify the synthetic data to simulate extreme market conditions, such as a crash or high volatility.\n",
    "\n",
    "# Generate synthetic data\n",
    "noise = np.random.normal(0, 1, (1000, 100))\n",
    "synthetic_prices = generator.predict(noise)\n",
    "\n",
    "# Simulate a crash (e.g., 50% drop)\n",
    "crash_prices = synthetic_prices * 0.5\n",
    "\n",
    "# Simulate high volatility (add random noise)\n",
    "volatile_prices = synthetic_prices + np.random.normal(0, 0.1, synthetic_prices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Perform Stress Testing\n",
    "# Use the synthetic data to test a financial model or portfolio. \n",
    "# For example, calculate the portfolio value under extreme conditions.\n",
    "# Example: Calculate portfolio value under crash scenario\n",
    "initial_portfolio_value = 1000000  # $1,000,000\n",
    "portfolio_value_crash = initial_portfolio_value * (1 + crash_prices)\n",
    "print(\"Portfolio values under crash scenario:\", portfolio_value_crash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Analyze Results\n",
    "# Evaluate the impact of the simulated scenarios on the portfolio or financial model.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Real vs Synthetic Prices Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(scaler.inverse_transform(scaled_prices), label=\"Real Prices\")\n",
    "plt.plot(scaler.inverse_transform(synthetic_prices), label=\"Synthetic Prices\")\n",
    "plt.plot(scaler.inverse_transform(crash_prices), label=\"Crash Scenario\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.title(\"Real vs Synthetic Prices\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.show()\n",
    "\n",
    "# Portfolio Value Under Crash Scenario\n",
    "portfolio_value_crash = initial_portfolio_value * (1 + crash_prices)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(portfolio_value_crash, label=\"Portfolio Value Under Crash\")\n",
    "plt.legend()\n",
    "plt.title(\"Portfolio Value Under Crash Scenario\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Portfolio Value ($)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR):\n",
    "# Calculate and plot VaR and CVaR to quantify the potential losses under extreme scenarios.\n",
    "\n",
    "import numpy as np\n",
    "losses = initial_portfolio_value - portfolio_value_crash\n",
    "var = np.percentile(losses, 95)  # 95% VaR\n",
    "cvar = losses[losses >= var].mean()  # 95% CVaR\n",
    "print(f\"95% VaR: ${var:.2f}, 95% CVaR: ${cvar:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
